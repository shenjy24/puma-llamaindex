import os
from typing import List, Optional

# ç”Ÿäº§çº§åˆ«çš„RAGæœåŠ¡ç¤ºä¾‹

# LlamaIndex v0.10+ æ ¸å¿ƒç»„ä»¶
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    StorageContext,
    SimpleDirectoryReader,
    Document,
    get_response_synthesizer,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SentenceTransformerRerank

# å‘é‡åº“ä¸åµŒå…¥æ¨¡å‹
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.llms.openai import OpenAI
import qdrant_client


class ProductionRAGService:
    def __init__(
        self,
        collection_name: str = "production_rag_hybrid",
        qdrant_url: str = "http://localhost:6333",
        openai_api_key: str = None,
    ):
        """
        åˆå§‹åŒ– RAG æœåŠ¡ï¼Œé…ç½®æ··åˆæ£€ç´¢ä¸é‡æ’åº
        """
        # 1. å…¨å±€æ¨¡å‹é…ç½® (ä½¿ç”¨ v0.10+ Settings)
        if openai_api_key:
            os.environ["OPENAI_API_KEY"] = openai_api_key

        Settings.llm = OpenAI(model="gpt-4o", temperature=0.1)
        Settings.embed_model = OpenAIEmbedding(model="text-embedding-3-small")

        # 2. æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥ (Chunking)
        # ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼šå—å¤§ä¸€äº›ä»¥ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨æ£€ç´¢æ—¶åˆ‡åˆ†æ›´ç»†æˆ–ä½¿ç”¨ overlap
        Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)

        # 3. åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯ (ç”Ÿäº§çº§å‘é‡æ•°æ®åº“)
        # æ³¨æ„ï¼šenable_hybrid=True å¼€å¯ç¨€ç–å‘é‡ç´¢å¼•ï¼Œfastembed_sparse_model æŒ‡å®šç¨€ç–æ¨¡å‹
        self.client = qdrant_client.QdrantClient(url=qdrant_url)
        self.vector_store = QdrantVectorStore(
            client=self.client,
            collection_name=collection_name,
            enable_hybrid=True,
            fastembed_sparse_model="Qdrant/bm25",  # ä½¿ç”¨è½»é‡çº§ BM25 æ¨¡å‹ç”Ÿæˆç¨€ç–å‘é‡
        )

        # 4. åˆå§‹åŒ–é‡æ’åºæ¨¡å‹ (Re-ranker)
        # Cross-Encoder æ¯”å•çº¯å‘é‡ç›¸ä¼¼åº¦æ›´ç²¾å‡†ï¼Œä½†è®¡ç®—è¾ƒæ…¢ï¼Œç”¨äºç¬¬äºŒé˜¶æ®µç­›é€‰
        # ä¹Ÿå¯ä»¥ä½¿ç”¨ CohereRerank (éœ€ API Key)
        self.reranker = SentenceTransformerRerank(
            model="cross-encoder/ms-marco-MiniLM-L-6-v2",
            top_n=3,  # æœ€ç»ˆç»™ LLM çš„ä¸Šä¸‹æ–‡æ•°é‡
        )

        self.index = self._load_or_create_index()

    def _load_or_create_index(self) -> VectorStoreIndex:
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•çš„å®¹å™¨"""
        storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

        # å°è¯•ä»å‘é‡åº“åŠ è½½ç´¢å¼• (ä¸é‡æ–°è®¡ç®— embedding)
        try:
            index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_store, storage_context=storage_context
            )
            print("âœ… å·²è¿æ¥åˆ°ç°æœ‰çš„æŒä¹…åŒ–å‘é‡ç´¢å¼•ã€‚")
            return index
        except Exception as e:
            print(f"â„¹ï¸ åˆå§‹åŒ–ç©ºç´¢å¼•: {e}")
            return VectorStoreIndex.from_documents([], storage_context=storage_context)

    def ingest_documents(self, data_dir: str):
        """
        æ•°æ®æ‘„å…¥ç®¡é“ï¼šè¯»å– -> åˆ‡åˆ† -> åµŒå…¥ -> å­˜å‚¨
        """
        print(f"ğŸ“‚ æ­£åœ¨ä» {data_dir} è¯»å–æ–‡æ¡£...")
        documents = SimpleDirectoryReader(data_dir).load_data()

        # ä½¿ç”¨ IngestionPipeline å¤„ç†å»é‡å’Œè½¬æ¢
        pipeline = IngestionPipeline(
            transformations=[
                Settings.node_parser,
                Settings.embed_model,
            ],
            vector_store=self.vector_store,
        )

        # è¿è¡Œç®¡é“ (è®¡ç®— embedding å¹¶å­˜å…¥ Qdrant)
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨è®¡ç®— Dense Vector (OpenAI) å’Œ Sparse Vector (BM25)
        nodes = pipeline.run(documents=documents)
        print(f"ğŸ‰ æˆåŠŸç´¢å¼• {len(nodes)} ä¸ªèŠ‚ç‚¹åˆ° Qdrantã€‚")

    def query(self, query_text: str) -> str:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢ï¼šæ··åˆæ£€ç´¢ -> é‡æ’åº -> LLM åˆæˆ
        """
        # 1. é…ç½®æ··åˆæ£€ç´¢å™¨ (Hybrid Retriever)
        # alpha å‚æ•°æ§åˆ¶æƒé‡ï¼š0.5 è¡¨ç¤º 50% å‘é‡æœç´¢ + 50% å…³é”®è¯æœç´¢
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,  # å¬å›æ›´å¤šæ–‡æ¡£ç”¨äºé‡æ’åº (ä¾‹å¦‚ 10 ä¸ª)
            vector_store_query_mode="hybrid",
            sparse_top_k=10,
            alpha=0.5,
        )

        # 2. æ„å»ºæŸ¥è¯¢å¼•æ“
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[self.reranker],  # åœ¨æ­¤å¤„åŠ å…¥é‡æ’åº
            response_synthesizer=get_response_synthesizer(response_mode="compact"),
        )

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        response = query_engine.query(query_text)

        # (å¯é€‰) æ‰“å°æ£€ç´¢åˆ°çš„æ¥æºä»¥ä¾›è°ƒè¯•
        # for node in response.source_nodes:
        #     print(f"Debug Source: {node.score:.4f} - {node.text[:50]}...")

        return str(response)


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":
    # å‡è®¾ä½ å·²ç»åœ¨æœ¬åœ°å¯åŠ¨äº† Qdrant (docker run -p 6333:6333 qdrant/qdrant)

    rag_service = ProductionRAGService(
        openai_api_key="sk-proj-...",  # æ›¿æ¢ä¸ºä½ çš„ Key
        qdrant_url="http://localhost:6333",
    )

    # 1. é¦–æ¬¡è¿è¡Œæ—¶æ‘„å…¥æ•°æ®
    # rag_service.ingest_documents("./data")

    # 2. æé—®
    answer = rag_service.query("LlamaIndex çš„æ··åˆæ£€ç´¢æ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Ÿ")
    print(f"\nğŸ¤– å›ç­”:\n{answer}")
