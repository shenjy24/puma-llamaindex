# ç”Ÿäº§çº§åˆ«çš„RAGæœåŠ¡ç¤ºä¾‹: ä½¿ç”¨ç¡…åŸºæµåŠ¨çš„æ¨¡å‹
import os
from dotenv import load_dotenv
from typing import Any, List

# LlamaIndex v0.10+ æ ¸å¿ƒç»„ä»¶
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    StorageContext,
    SimpleDirectoryReader,
    get_response_synthesizer,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.vector_stores.qdrant import QdrantVectorStore
import qdrant_client

import requests
from typing import List, Optional
from llama_index.core.bridge.pydantic import Field, PrivateAttr
from llama_index.core.postprocessor.types import BaseNodePostprocessor
from llama_index.core.schema import NodeWithScore, QueryBundle

import openai
from llama_index.llms.openai_like import OpenAILike
from llama_index.core.embeddings import BaseEmbedding
from llama_index.core.chat_engine import ContextChatEngine
from llama_index.core.memory import ChatMemoryBuffer

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# è¿™è¡Œä»£ç ä¼šæŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶å¹¶å°†å˜é‡æ³¨å…¥åˆ° os.environ ä¸­
load_dotenv()


class ProductionRAGService:
    def __init__(
        self,
        collection_name: str = "production_rag_hybrid",
        qdrant_url: str = "http://localhost:6333",
        qdrant_api_key: str = None,
        silicon_api_key: str = None,
        silicon_api_base: str = None,
    ):
        """
        åˆå§‹åŒ– RAG æœåŠ¡ï¼Œé…ç½®æ··åˆæ£€ç´¢ä¸é‡æ’åº
        """
        # 1. å…¨å±€æ¨¡å‹é…ç½® (ä½¿ç”¨ v0.10+ Settings)
        # =========================================================
        # é…ç½® Embedding (åµŒå…¥æ¨¡å‹) - ä½¿ç”¨ç¡…åŸºæµåŠ¨äº‘ç«¯ç‰ˆ BGE-M3
        # =========================================================
        # åŸæ¥çš„ HuggingFaceEmbedding æ˜¯æœ¬åœ°è·‘ï¼Œç°åœ¨æ”¹ç”¨ OpenAIEmbedding è°ƒäº‘ç«¯ API
        # ç¡…åŸºæµåŠ¨æ”¯æŒçš„æ¨¡å‹ ID ä¸º: "BAAI/bge-m3"
        Settings.embed_model = SiliconFlowEmbedding(
            model_name="BAAI/bge-m3",
            api_key=silicon_api_key,
            api_base=silicon_api_base,
        )

        # =========================================================
        # é…ç½® LLM (å¤§è¯­è¨€æ¨¡å‹) - ä½¿ç”¨ç¡…åŸºæµåŠ¨ DeepSeek-V3
        # =========================================================
        # æ³¨æ„ï¼šç¡…åŸºæµåŠ¨çš„ DeepSeek V3 æ¨¡å‹ ID é€šå¸¸æ˜¯ "deepseek-ai/DeepSeek-V3"
        # å¦‚æœä½ æƒ³ç”¨ R1ï¼Œå°±æ”¹æˆ "deepseek-ai/DeepSeek-R1"
        Settings.llm = OpenAILike(
            model="deepseek-ai/DeepSeek-V3",
            api_base=silicon_api_base,
            api_key=silicon_api_key,
            is_chat_model=True,
            # --- RAG æ ¸å¿ƒä¼˜åŒ–å‚æ•° (ä¿æŒä¸å˜) ---
            # 1. æ¸©åº¦: æä½ï¼Œå‡å°‘å¹»è§‰
            temperature=0.0,
            # 2. ä¸Šä¸‹æ–‡çª—å£: å³ä½¿æ˜¯ OpenAI ç±»ï¼Œæœ€å¥½ä¹Ÿæ˜¾å¼å£°æ˜ï¼Œé˜²æ­¢åº“é»˜è®¤ä½¿ç”¨ GPT-3.5 çš„ 4k é™åˆ¶
            # å‘Šè¯‰ LlamaIndex è¿™ä¸ªæ¨¡å‹èƒ½åƒ 60k token
            context_window=60000,
            # 3. æœ€å¤§è¾“å‡º
            max_tokens=4096,
            # 4. é‡è¯•æœºåˆ¶
            max_retries=3,
            # 5. é¢å¤–å‚æ•°
            additional_kwargs={
                "top_p": 0.95,
            },
            # 6. (å¯é€‰) è®¾ä¸º True å¯ä»¥è®© LlamaIndex å¤ç”¨ API è¿æ¥ï¼Œæå‡ä¸€ç‚¹ç‚¹é€Ÿåº¦
            reuse_client=True,
        )

        # 2. æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥ (Chunking)
        # ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼šå—å¤§ä¸€äº›ä»¥ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨æ£€ç´¢æ—¶åˆ‡åˆ†æ›´ç»†æˆ–ä½¿ç”¨ overlap
        Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)

        # 3. åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯ (ç”Ÿäº§çº§å‘é‡æ•°æ®åº“)
        # æ³¨æ„ï¼šenable_hybrid=True å¼€å¯ç¨€ç–å‘é‡ç´¢å¼•ï¼Œfastembed_sparse_model æŒ‡å®šç¨€ç–æ¨¡å‹
        self.client = qdrant_client.QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
        self.vector_store = QdrantVectorStore(
            client=self.client,
            collection_name=collection_name,
            enable_hybrid=True,
            fastembed_sparse_model="Qdrant/bm25",  # ä½¿ç”¨è½»é‡çº§ BM25 æ¨¡å‹ç”Ÿæˆç¨€ç–å‘é‡
        )

        # 4. åˆå§‹åŒ–é‡æ’åºæ¨¡å‹ (Re-ranker)
        # ==========================================
        # ä½¿ç”¨æ–¹æ³•ï¼šæ›¿æ¢æ‰ä½ åŸæ¥çš„ SentenceTransformerRerank
        # ==========================================
        # ç¡…åŸºæµåŠ¨ç›®å‰æ”¯æŒçš„æ¨¡å‹ ID æ˜¯ "BAAI/bge-reranker-v2-m3"
        self.reranker = SiliconFlowRerank(
            model="BAAI/bge-reranker-v2-m3",
            api_key=silicon_api_key,
            top_n=3,
        )

        # è®°å¿†ç»„ä»¶
        self.memory = ChatMemoryBuffer.from_defaults(token_limit=3900)

        self.index = self._load_or_create_index()

    def _load_or_create_index(self) -> VectorStoreIndex:
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•çš„å®¹å™¨"""
        storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

        # å°è¯•ä»å‘é‡åº“åŠ è½½ç´¢å¼• (ä¸é‡æ–°è®¡ç®— embedding)
        try:
            index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_store, storage_context=storage_context
            )
            print("âœ… å·²è¿æ¥åˆ°ç°æœ‰çš„æŒä¹…åŒ–å‘é‡ç´¢å¼•ã€‚")
            return index
        except Exception as e:
            print(f"â„¹ï¸ åˆå§‹åŒ–ç©ºç´¢å¼•: {e}")
            return VectorStoreIndex.from_documents([], storage_context=storage_context)

    def ingest_documents(self, data_dir: str):
        """
        æ•°æ®æ‘„å…¥ç®¡é“ï¼šè¯»å– -> åˆ‡åˆ† -> åµŒå…¥ -> å­˜å‚¨
        """
        print(f"ğŸ“‚ æ­£åœ¨ä» {data_dir} è¯»å–æ–‡æ¡£...")
        documents = SimpleDirectoryReader(data_dir).load_data()

        # ä½¿ç”¨ IngestionPipeline å¤„ç†å»é‡å’Œè½¬æ¢
        pipeline = IngestionPipeline(
            transformations=[
                Settings.node_parser,
                Settings.embed_model,
            ],
            vector_store=self.vector_store,
        )

        # è¿è¡Œç®¡é“ (è®¡ç®— embedding å¹¶å­˜å…¥ Qdrant)
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨è®¡ç®— Dense Vector (OpenAI) å’Œ Sparse Vector (BM25)
        nodes = pipeline.run(documents=documents)
        print(f"ğŸ‰ æˆåŠŸç´¢å¼• {len(nodes)} ä¸ªèŠ‚ç‚¹åˆ° Qdrantã€‚")

    def query(self, query_text: str) -> str:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢ï¼šæ··åˆæ£€ç´¢ -> é‡æ’åº -> LLM åˆæˆ
        """
        # 1. é…ç½®æ··åˆæ£€ç´¢å™¨ (Hybrid Retriever)
        # alpha å‚æ•°æ§åˆ¶æƒé‡ï¼š0.5 è¡¨ç¤º 50% å‘é‡æœç´¢ + 50% å…³é”®è¯æœç´¢
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,  # å¬å›æ›´å¤šæ–‡æ¡£ç”¨äºé‡æ’åº (ä¾‹å¦‚ 10 ä¸ª)
            vector_store_query_mode="hybrid",
            sparse_top_k=10,
            alpha=0.5,
        )

        # 2. æ„å»ºæŸ¥è¯¢å¼•æ“
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[self.reranker],  # åœ¨æ­¤å¤„åŠ å…¥é‡æ’åº
            response_synthesizer=get_response_synthesizer(response_mode="compact"),
        )

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        response = query_engine.query(query_text)

        # (å¯é€‰) æ‰“å°æ£€ç´¢åˆ°çš„æ¥æºä»¥ä¾›è°ƒè¯•
        # for node in response.source_nodes:
        #     print(f"Debug Source: {node.score:.4f} - {node.text[:50]}...")

        return str(response)

    def stream_query(self, query_text: str):
        """
        æ‰§è¡Œæµå¼ RAG æŸ¥è¯¢ï¼šæ··åˆæ£€ç´¢ -> é‡æ’åº -> LLM æµå¼åˆæˆ
        è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼Œé€ä¸ª token è¾“å‡ºå“åº”
        """
        # ç«‹å³ç»™ç”¨æˆ·åé¦ˆï¼ˆéå¸¸é‡è¦ï¼‰
        yield "ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™...\n"

        # é…ç½®æ··åˆæ£€ç´¢å™¨ (Hybrid Retriever)
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,
            vector_store_query_mode="hybrid",
            sparse_top_k=10,
            alpha=0.5,
        )

        # æ„å»ºæŸ¥è¯¢å¼•æ“ (ä½¿ç”¨æµå¼åˆæˆå™¨)
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[self.reranker],
            response_synthesizer=get_response_synthesizer(
                response_mode="compact", streaming=True
            ),
        )

        yield "ğŸ§  æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...\n\n"

        # æ‰§è¡Œæµå¼æŸ¥è¯¢
        try:
            response = query_engine.query(query_text)
        except Exception as e:
            yield f"âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"
            return

        # ç¨³å®šæµå¼è¾“å‡º
        if hasattr(response, "response_gen") and response.response_gen:
            for token in response.response_gen:
                yield token
        else:
            # é™çº§å…œåº•ï¼ˆæå°‘å‘ç”Ÿï¼‰
            yield str(response)

    def stream_memory_query(self, query_text: str):
        """
        æ‰§è¡Œå¸¦è®°å¿†åŠŸèƒ½çš„æµå¼ RAG æŸ¥è¯¢
        """
        # ç«‹å³ç»™ç”¨æˆ·åé¦ˆï¼ˆéå¸¸é‡è¦ï¼‰
        yield "ğŸ” æ­£åœ¨æ£€ç´¢ç›¸å…³èµ„æ–™...\n"

        # é…ç½®æ··åˆæ£€ç´¢å™¨ (Hybrid Retriever)
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,
            vector_store_query_mode="hybrid",
            sparse_top_k=10,
            alpha=0.5,
        )

        # æ„å»º ContextChatEngine
        # å®ƒä¼šè‡ªåŠ¨å¤„ç†ï¼šå†å²å¯¹è¯é‡å†™ + çŸ¥è¯†æ£€ç´¢ + ç­”æ¡ˆåˆæˆ
        chat_engine = ContextChatEngine.from_defaults(
            retriever=retriever,
            node_postprocessors=[self.reranker],
            memory=self.memory,
            system_prompt="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŠ©æ‰‹ã€‚è¯·ç»“åˆç»™å®šçš„æœ¬åœ°çŸ¥è¯†åº“å’Œå¯¹è¯å†å²æ¥å›ç­”é—®é¢˜ã€‚",
        )

        yield "ğŸ§  æ­£åœ¨ç”Ÿæˆç­”æ¡ˆ...\n\n"

        # æ‰§è¡Œæµå¼æŸ¥è¯¢
        try:
            response = chat_engine.stream_chat(query_text)
        except Exception as e:
            yield f"âš ï¸ æŸ¥è¯¢å¤±è´¥ï¼š{str(e)}"
            return

        # ç¨³å®šæµå¼è¾“å‡º
        if hasattr(response, "response_gen") and response.response_gen:
            for token in response.response_gen:
                yield token
        else:
            # é™çº§å…œåº•ï¼ˆæå°‘å‘ç”Ÿï¼‰
            yield str(response)


class SiliconFlowEmbedding(BaseEmbedding):
    """
    ä¸“é—¨ä¸ºç¡…åŸºæµåŠ¨ (SiliconFlow) å®šåˆ¶çš„ Embedding ç±»
    ç»•è¿‡ LlamaIndex å¯¹ OpenAI æ¨¡å‹åç§°çš„å¼ºåˆ¶æ ¡éªŒ
    """

    _client: openai.Client = PrivateAttr()
    _model_name: str = PrivateAttr()

    def __init__(
        self,
        model_name: str = "BAAI/bge-m3",
        api_key: str = None,
        api_base: str = "https://api.siliconflow.cn/v1",
        **kwargs: Any,
    ):
        super().__init__(model_name=model_name, **kwargs)
        self._model_name = model_name
        # åˆå§‹åŒ–æ ‡å‡†çš„ OpenAI å®¢æˆ·ç«¯
        self._client = openai.Client(api_key=api_key, base_url=api_base)

    def _get_query_embedding(self, query: str) -> List[float]:
        """è·å–å•ä¸ªæŸ¥è¯¢çš„ embedding"""
        return self._get_embedding(query)

    def _get_text_embedding(self, text: str) -> List[float]:
        """è·å–å•ä¸ªæ–‡æ¡£ç‰‡æ®µçš„ embedding"""
        return self._get_embedding(text)

    def _get_text_embeddings(self, texts: List[str]) -> List[List[float]]:
        """
        æ‰¹é‡è·å– embedding (å…³é”®ä¼˜åŒ–ï¼šå‡å°‘ç½‘ç»œè¯·æ±‚æ¬¡æ•°)
        """
        # ç§»é™¤æ¢è¡Œç¬¦æ˜¯ embedding çš„æœ€ä½³å®è·µ
        texts = [t.replace("\n", " ") for t in texts]
        try:
            response = self._client.embeddings.create(
                input=texts, model=self._model_name
            )
            # æŒ‰ç…§è¿”å›é¡ºåºæå– embedding
            return [data.embedding for data in response.data]
        except Exception as e:
            print(f"Error generating embeddings: {e}")
            raise e

    def _get_embedding(self, text: str) -> List[float]:
        """å†…éƒ¨é€šç”¨æ–¹æ³•"""
        text = text.replace("\n", " ")
        response = self._client.embeddings.create(input=[text], model=self._model_name)
        return response.data[0].embedding

    async def _aget_query_embedding(self, query: str) -> List[float]:
        # ç®€å•èµ·è§ï¼Œæš‚ä¸å®ç°å¼‚æ­¥ï¼Œç›´æ¥è°ƒåŒæ­¥æ–¹æ³•
        return self._get_query_embedding(query)


class SiliconFlowRerank(BaseNodePostprocessor):
    """
    è‡ªå®šä¹‰çš„ç¡…åŸºæµåŠ¨ Rerank å¤„ç†å™¨
    """

    siliconflow_api_base: str = os.getenv("SILICONFLOW_API_BASE", "")
    model: str = Field(description="Rerank model name")
    top_n: int = Field(description="Top N nodes to return")
    api_key: str = Field(description="SiliconFlow API Key")
    base_url: str = Field(default=siliconflow_api_base, description="API Endpoint")

    def _postprocess_nodes(
        self,
        nodes: List[NodeWithScore],
        query_bundle: Optional[QueryBundle] = None,
    ) -> List[NodeWithScore]:
        if not nodes:
            return []

        request_url = self.base_url
        if not request_url.endswith("/rerank"):
            # ç®€å•çš„æ‹¼æ¥å¤„ç†ï¼Œé˜²æ­¢ç”¨æˆ·åªå¡«äº† base åŸŸå
            if request_url.endswith("/v1"):
                request_url = f"{request_url}/rerank"
            else:
                request_url = f"{request_url}/v1/rerank"

        # å‡†å¤‡è¯·æ±‚æ•°æ®
        documents = [node.node.get_content() for node in nodes]
        payload = {
            "model": self.model,
            "query": query_bundle.query_str,
            "documents": documents,
            "top_n": self.top_n,
            "return_documents": False,
        }
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json",
        }

        # å‘é€è¯·æ±‚
        try:
            response = requests.post(request_url, json=payload, headers=headers)
            response.raise_for_status()
            results = response.json().get("results", [])

            # æ ¹æ®è¿”å›çš„ index é‡æ–°æ’åºå¹¶èµ‹å€¼åˆ†æ•°
            new_nodes = []
            for res in results:
                idx = res["index"]
                score = res["relevance_score"]

                node = nodes[idx]
                node.score = score  # æ›´æ–°åˆ†æ•°ä¸º Cross-Encoder çš„ç²¾å‡†åˆ†æ•°
                new_nodes.append(node)

            return new_nodes

        except Exception as e:
            print(f"Rerank API Error: {e}")
            # å¦‚æœ API æŒ‚äº†ï¼Œé™çº§è¿”å›åŸæ¥çš„å‰ N ä¸ªï¼Œé˜²æ­¢ç¨‹åºå´©æºƒ
            return nodes[: self.top_n]


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":

    silicon_api_key: str = os.getenv("SILICONFLOW_API_KEY", "")
    silicon_api_base = os.getenv("SILICONFLOW_API_BASE", "")
    qdrant_api_key: str = os.getenv("QDRANT_API_KEY", "")
    qdrant_api_base: str = os.getenv("QDRANT_API_BASE", "")

    rag_service = ProductionRAGService(
        qdrant_url=qdrant_api_base,
        qdrant_api_key=qdrant_api_key,
        silicon_api_key=silicon_api_key,
        silicon_api_base=silicon_api_base,
    )

    # 1. é¦–æ¬¡è¿è¡Œæ—¶æ‘„å…¥æ•°æ®
    # base_dir = os.path.dirname(os.path.abspath(__file__))
    # data_dir = os.path.join(base_dir, "data")
    # rag_service.ingest_documents(data_dir)

    # 2. æµå¼æé—®ç¤ºä¾‹
    stream_generator = rag_service.stream_query(
        "PDFBox æä¾›çš„ä¸€äº›å…³é”®åŠŸèƒ½å’ŒåŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ"
    )
    for token in stream_generator:
        print(token, end="", flush=True)
    print("\n")

    # 3. éæµå¼æé—®ç¤ºä¾‹ (å¦‚éœ€è¦)
    # answer = rag_service.query("PDFBox æä¾›çš„ä¸€äº›å…³é”®åŠŸèƒ½å’ŒåŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ")
    # print(f"\nğŸ¤– å›ç­”:\n{answer}")
