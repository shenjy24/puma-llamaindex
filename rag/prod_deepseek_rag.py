# ç”Ÿäº§çº§åˆ«çš„RAGæœåŠ¡ç¤ºä¾‹
import os

# å¿…é¡»æ”¾åœ¨ç¬¬ä¸€è¡Œï¼Œä»»ä½•å…¶ä»– import ä¹‹å‰
os.environ["HF_ENDPOINT"] = "https://hf-mirror.com"
from typing import List, Optional
from dotenv import load_dotenv

# LlamaIndex v0.10+ æ ¸å¿ƒç»„ä»¶
from llama_index.core import (
    Settings,
    VectorStoreIndex,
    StorageContext,
    SimpleDirectoryReader,
    Document,
    get_response_synthesizer,
)
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.ingestion import IngestionPipeline
from llama_index.core.query_engine import RetrieverQueryEngine
from llama_index.core.retrievers import VectorIndexRetriever
from llama_index.core.postprocessor import SentenceTransformerRerank

# å‘é‡åº“ä¸åµŒå…¥æ¨¡å‹
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.llms.openai import OpenAI
from llama_index.llms.deepseek import DeepSeek
import qdrant_client

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
# è¿™è¡Œä»£ç ä¼šæŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„ .env æ–‡ä»¶å¹¶å°†å˜é‡æ³¨å…¥åˆ° os.environ ä¸­
load_dotenv()

class ProductionRAGService:
    def __init__(
        self,
        collection_name: str = "production_rag_hybrid",
        qdrant_url: str = "http://localhost:6333",
        qdrant_api_key: str = None,
        deepseek_api_key: str = None,
        deepseek_api_base: str = None,
        deepseek_model_name: str = None,
    ):
        """
        åˆå§‹åŒ– RAG æœåŠ¡ï¼Œé…ç½®æ··åˆæ£€ç´¢ä¸é‡æ’åº
        """
        # 1. å…¨å±€æ¨¡å‹é…ç½® (ä½¿ç”¨ v0.10+ Settings)
        Settings.embed_model = HuggingFaceEmbedding(model_name="BAAI/bge-m3")
        Settings.llm = DeepSeek(
            model=deepseek_model_name,
            api_base=deepseek_api_base,
            api_key=deepseek_api_key,
            # --- RAG æ ¸å¿ƒä¼˜åŒ–å‚æ•° ---
            # 1. æ¸©åº¦ (Temperature): æä½
            # RAG ä¸éœ€è¦æ¨¡å‹"å‘æ•£æ€ç»´"ï¼Œåªéœ€è¦å®ƒ"ç…§ç€æ–‡æ¡£è¯´"ã€‚
            # 0.0 æˆ– 0.1 æ˜¯æœ€ä½³é€‰æ‹©ï¼Œèƒ½æœ€å¤§ç¨‹åº¦å‡å°‘å¹»è§‰ã€‚
            temperature=0.0,
            # 2. ä¸Šä¸‹æ–‡çª—å£ (Context Window): æ˜¾å¼å£°æ˜
            # DeepSeek æ”¯æŒ 64k+ï¼Œä½† LlamaIndex é»˜è®¤å¯èƒ½åªè¯†åˆ«ä¸º 4kã€‚
            # å¿…é¡»æ‰‹åŠ¨è®¾ç½®å¤§ä¸€ç‚¹ï¼Œè¿™æ · RAG æ‰èƒ½ä¸€æ¬¡æ€§å¡å…¥æ›´å¤šæ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µã€‚
            context_window=60000,
            # 3. æœ€å¤§è¾“å‡º Token (Max Tokens)
            # ç¨å¾®è°ƒå¤§ï¼Œé˜²æ­¢å›ç­”é•¿é—®é¢˜æ—¶è¢«æˆªæ–­ã€‚
            max_tokens=4096,
            # 4. é‡è¯•æœºåˆ¶ (Max Retries)
            # DeepSeek API å¶å°”ä¼šæœ‰å¹¶å‘é™åˆ¶ï¼Œ3-5 æ¬¡é‡è¯•æ¯”è¾ƒç¨³å¦¥ã€‚
            max_retries=3,
            # 5. é¢å¤–å‚æ•° (Optional)
            # å¦‚æœæ˜¯ DeepSeek-V3ï¼Œå¯ä»¥å¼ºåˆ¶æŒ‡å®š top_p æ¥è¿›ä¸€æ­¥æ”¶æ•›ç»“æœ
            additional_kwargs={
                "top_p": 0.95,  # é…åˆä½ temperatureï¼Œè¿›ä¸€æ­¥è¿‡æ»¤ä½æ¦‚ç‡çš„ç¦»è°±å›ç­”
                # "response_format": {"type": "json_object"} # å¦‚æœä½ çš„ RAG éœ€è¦è¾“å‡ºçº¯ JSONï¼ŒæŠŠè¿™è¡Œè§£å¼€
            },
        )

        # 2. æ–‡æœ¬åˆ‡åˆ†ç­–ç•¥ (Chunking)
        # ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼šå—å¤§ä¸€äº›ä»¥ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œä½†åœ¨æ£€ç´¢æ—¶åˆ‡åˆ†æ›´ç»†æˆ–ä½¿ç”¨ overlap
        Settings.node_parser = SentenceSplitter(chunk_size=512, chunk_overlap=50)

        # 3. åˆå§‹åŒ– Qdrant å®¢æˆ·ç«¯ (ç”Ÿäº§çº§å‘é‡æ•°æ®åº“)
        # æ³¨æ„ï¼šenable_hybrid=True å¼€å¯ç¨€ç–å‘é‡ç´¢å¼•ï¼Œfastembed_sparse_model æŒ‡å®šç¨€ç–æ¨¡å‹
        self.client = qdrant_client.QdrantClient(url=qdrant_url, api_key=qdrant_api_key)
        self.vector_store = QdrantVectorStore(
            client=self.client,
            collection_name=collection_name,
            enable_hybrid=True,
            fastembed_sparse_model="Qdrant/bm25",  # ä½¿ç”¨è½»é‡çº§ BM25 æ¨¡å‹ç”Ÿæˆç¨€ç–å‘é‡
        )

        # 4. åˆå§‹åŒ–é‡æ’åºæ¨¡å‹ (Re-ranker)
        # Cross-Encoder æ¯”å•çº¯å‘é‡ç›¸ä¼¼åº¦æ›´ç²¾å‡†ï¼Œä½†è®¡ç®—è¾ƒæ…¢ï¼Œç”¨äºç¬¬äºŒé˜¶æ®µç­›é€‰
        # ä¹Ÿå¯ä»¥ä½¿ç”¨ CohereRerank (éœ€ API Key)
        self.reranker = SentenceTransformerRerank(
            model="cross-encoder/ms-marco-MiniLM-L-6-v2",
            top_n=3,  # æœ€ç»ˆç»™ LLM çš„ä¸Šä¸‹æ–‡æ•°é‡
        )

        self.index = self._load_or_create_index()

    def _load_or_create_index(self) -> VectorStoreIndex:
        """åŠ è½½ç°æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼•çš„å®¹å™¨"""
        storage_context = StorageContext.from_defaults(vector_store=self.vector_store)

        # å°è¯•ä»å‘é‡åº“åŠ è½½ç´¢å¼• (ä¸é‡æ–°è®¡ç®— embedding)
        try:
            index = VectorStoreIndex.from_vector_store(
                vector_store=self.vector_store, storage_context=storage_context
            )
            print("âœ… å·²è¿æ¥åˆ°ç°æœ‰çš„æŒä¹…åŒ–å‘é‡ç´¢å¼•ã€‚")
            return index
        except Exception as e:
            print(f"â„¹ï¸ åˆå§‹åŒ–ç©ºç´¢å¼•: {e}")
            return VectorStoreIndex.from_documents([], storage_context=storage_context)

    def ingest_documents(self, data_dir: str):
        """
        æ•°æ®æ‘„å…¥ç®¡é“ï¼šè¯»å– -> åˆ‡åˆ† -> åµŒå…¥ -> å­˜å‚¨
        """
        print(f"ğŸ“‚ æ­£åœ¨ä» {data_dir} è¯»å–æ–‡æ¡£...")
        documents = SimpleDirectoryReader(data_dir).load_data()

        # ä½¿ç”¨ IngestionPipeline å¤„ç†å»é‡å’Œè½¬æ¢
        pipeline = IngestionPipeline(
            transformations=[
                Settings.node_parser,
                Settings.embed_model,
            ],
            vector_store=self.vector_store,
        )

        # è¿è¡Œç®¡é“ (è®¡ç®— embedding å¹¶å­˜å…¥ Qdrant)
        # è¿™ä¸€æ­¥ä¼šè‡ªåŠ¨è®¡ç®— Dense Vector (OpenAI) å’Œ Sparse Vector (BM25)
        nodes = pipeline.run(documents=documents)
        print(f"ğŸ‰ æˆåŠŸç´¢å¼• {len(nodes)} ä¸ªèŠ‚ç‚¹åˆ° Qdrantã€‚")

    def query(self, query_text: str) -> str:
        """
        æ‰§è¡Œ RAG æŸ¥è¯¢ï¼šæ··åˆæ£€ç´¢ -> é‡æ’åº -> LLM åˆæˆ
        """
        # 1. é…ç½®æ··åˆæ£€ç´¢å™¨ (Hybrid Retriever)
        # alpha å‚æ•°æ§åˆ¶æƒé‡ï¼š0.5 è¡¨ç¤º 50% å‘é‡æœç´¢ + 50% å…³é”®è¯æœç´¢
        retriever = VectorIndexRetriever(
            index=self.index,
            similarity_top_k=10,  # å¬å›æ›´å¤šæ–‡æ¡£ç”¨äºé‡æ’åº (ä¾‹å¦‚ 10 ä¸ª)
            vector_store_query_mode="hybrid",
            sparse_top_k=10,
            alpha=0.5,
        )

        # 2. æ„å»ºæŸ¥è¯¢å¼•æ“
        query_engine = RetrieverQueryEngine(
            retriever=retriever,
            node_postprocessors=[self.reranker],  # åœ¨æ­¤å¤„åŠ å…¥é‡æ’åº
            response_synthesizer=get_response_synthesizer(response_mode="compact"),
        )

        # 3. æ‰§è¡ŒæŸ¥è¯¢
        response = query_engine.query(query_text)

        # (å¯é€‰) æ‰“å°æ£€ç´¢åˆ°çš„æ¥æºä»¥ä¾›è°ƒè¯•
        # for node in response.source_nodes:
        #     print(f"Debug Source: {node.score:.4f} - {node.text[:50]}...")

        return str(response)


# --- ä½¿ç”¨ç¤ºä¾‹ ---
if __name__ == "__main__":

    deepseek_api_key: str = os.getenv("DEEPSEEK_API_KEY", "")
    deepseek_api_base = os.getenv("DEEPSEEK_API_BASE", "https://api.deepseek.com")
    deepseek_model_name = os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
    qdrant_api_key: str = os.getenv("QDRANT_API_KEY", "")

    rag_service = ProductionRAGService(
        qdrant_url="https://d5ba48cb-d32d-4d42-a111-6f70b300d550.europe-west3-0.gcp.cloud.qdrant.io",
        qdrant_api_key=qdrant_api_key,
        deepseek_api_key=deepseek_api_key,
        deepseek_api_base=deepseek_api_base,
        deepseek_model_name=deepseek_model_name
    )

    # 1. é¦–æ¬¡è¿è¡Œæ—¶æ‘„å…¥æ•°æ®
    base_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(base_dir, "data")
    rag_service.ingest_documents(data_dir)

    # 2. æé—®
    answer = rag_service.query("PDFBox æä¾›çš„ä¸€äº›å…³é”®åŠŸèƒ½å’ŒåŠŸèƒ½æœ‰å“ªäº›ï¼Ÿ")
    print(f"\nğŸ¤– å›ç­”:\n{answer}")
